https://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid1=105&sid2=230

https://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid2=230&sid1=105&date=20210119&page=2

https://news.naver.com/main/list.nhn?mode=LS2D&sid2=230&sid1=105&mid=shm&date=20210119&page=3

# ? 뒤의 정보가 의미하는 것 : 서버에 전달되는 파라미터의 정보(전달 파라미터)
# ? 앞의 정보가 의미하는 것 : 서버 주소에 대한 정보
# 전달 파라미터 (key) = (value)
                mode = LS2D 
                mid = shm
                sid1 = 105
                sid2 = 230


첫번째 페이지 기사 제목 - xpath 값

//*[@id="main_content"]/div[2]/ul[1]/li[1]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[1]/li[2]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[1]/li[3]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[1]/li[10]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[2]/li[1]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[2]/li[3]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[2]/li[10]/dl/dt[2]/a


-> 기사의 개수에 따라 li[n]값이 1씩 증가
-> 기사 개수가 10개를 넘어가면 ul[2]이 되고 li[1]으로 됨
-> 전체 기사의 xpath에 대해서 ul과 li의 값만 변화한다.

기사제목패턴 - xpath 의 패턴
//*[@id="main_content"]/div[2]/ul/li/dl/dt[2]/a

-> 이렇게 하면 각 페이지마다 모든 기사에 대한 xpath를 나타내는 것이다.



크롤링 타겟
1. 제목
2. 올린 뉴스 사이트
3. 미리보기 내용
4. 시간


scrapy shell

# 웹사이트 정보 추출
>>>fetch('웹사이트주소')

# 해당 웹사이트 브라우저에 띄우기
>>>view(resonse)

# 해당 웹사이트의 전체 소스 출력
>>> print(response.text)

# 웹사이트의 문서 추출
# 이때 xpath 값의 패턴을 입력하면 해당 페이지의 모든 기사 내용 추출
>>>response.xpath('xpath 값')

# 기사의 소속 뉴스 회사 이름만 추출
response.css('.writing::text').extract()

# 기사의 내용 미리보기만 추출
response.css('.lede::text').extract()

# 기타 원하는 내용만 추출하려면 '.html 파일의 class 이름::text'을 입력하면됨



scrapy project 구조

tutorial/
	scrapy.cfg

	tutorial/			# project's Python module. you'll import your code from here
		__init__.py

		items.py		# project items definition file

		middlewares.py	# project middlewares file

		pipelines.py

		settings.py

		spiders/__init__.py	# a directory where you'll later be 



anacoda prompt에서 scrapy project 만들고 실행

# scrapy 프로젝트 생성
>scrapy startproject [project_name]

프로젝트를 생성하면 같은 이름의 디렉터리가 만들어지고 그 내부에 프로젝트 파일들이 있음

# visual code 실행하여 디렉터리의 모든 코드를 띄움
> code .

# bot file 생성
> scrapy genspider [bot_name] ["스크래핑하려는 웹사이트 주소(http 프로토콜 제외)"]

# 코드 작성 후 실행
> scrapy crawl [bot_name]