https://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid1=105&sid2=230

https://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid2=230&sid1=105&date=20210119&page=2

https://news.naver.com/main/list.nhn?mode=LS2D&sid2=230&sid1=105&mid=shm&date=20210119&page=3

# ? 뒤의 정보가 의미하는 것 : 서버에 전달되는 파라미터의 정보(전달 파라미터)
# ? 앞의 정보가 의미하는 것 : 서버 주소에 대한 정보
# 전달 파라미터 (key) = (value)
                mode = LS2D 
                mid = shm
                sid1 = 105
                sid2 = 230


첫번째 페이지 기사 제목 - xpath 값

//*[@id="main_content"]/div[2]/ul[1]/li[1]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[1]/li[2]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[1]/li[3]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[1]/li[10]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[2]/li[1]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[2]/li[3]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[2]/li[10]/dl/dt[2]/a


-> 기사의 개수에 따라 li[n]값이 1씩 증가
-> 기사 개수가 10개를 넘어가면 ul[2]이 되고 li[1]으로 됨
-> 전체 기사의 xpath에 대해서 ul과 li의 값만 변화한다.

기사제목패턴 - xpath 의 패턴
//*[@id="main_content"]/div[2]/ul/li/dl/dt[2]/a

-> 이렇게 하면 각 페이지마다 모든 기사에 대한 xpath를 나타내는 것이다.



크롤링 타겟
1. 제목
2. 올린 뉴스 사이트
3. 미리보기 내용
4. 시간


scrapy shell

# 웹사이트 정보 추출
>>>fetch('웹사이트주소')

# 해당 웹사이트 브라우저에 띄우기
>>>view(resonse)

# 해당 웹사이트의 전체 소스 출력
>>> print(response.text)

# 웹사이트의 문서 추출
# 이때 xpath 값의 패턴을 입력하면 해당 페이지의 모든 기사 내용 추출
>>>response.xpath('xpath 값')

# 기사의 소속 뉴스 회사 이름만 추출
response.css('.writing::text').extract()

# 기사의 내용 미리보기만 추출
response.css('.lede::text').extract()

# 기타 원하는 내용만 추출하려면 '.html 파일의 class 이름::text'을 입력하면됨



scrapy project 구조

tutorial/
	scrapy.cfg

	tutorial/			# project's Python module. you'll import your code from here
		__init__.py

		items.py		# project items definition file

		middlewares.py	# project middlewares file

		pipelines.py

		settings.py

		spiders/__init__.py	# a directory where you'll later be 



anacoda prompt에서 scrapy project 만들고 실행

# scrapy 프로젝트 생성
>scrapy startproject [project_name]

프로젝트를 생성하면 같은 이름의 디렉터리가 만들어지고 그 내부에 프로젝트 파일들이 있음

# visual code 실행하여 디렉터리의 모든 코드를 띄움
> code .

# bot file 생성
> scrapy genspider [bot_name] ["스크래핑하려는 웹사이트 주소(http 프로토콜 제외)"]

# 코드 작성 후 실행
> scrapy crawl [bot_name]


---django project---
1. 프로젝트 생성 : django-admin startproject djangobookmark
2. 마이크레이션 : python manage.py migrate
3. superuser 생성 : python manage.py createsuperuser
4. 어플리케이션 생성 : python manage.py startapp bookmarkap
5. 마이그레이션 : python manage.py makemigrations
6. 서버 작동 : python manage.py runserver 호스트번호(없을시 자동 8000)


DB 확인
1. python manage.py dbshell

- .table = DB에 있는 테이블 보여줌
- select * from bookmarkap_bookmark; -> bookmarkap_bookmark의 모든 데이터를 선택하여 보여줌
- PRAGMA table_info(bookmarkap_bookmark); -> bookmarkap_bookmark 테이블의 정보를 보여줌(PRAGMA 해야함)
- select * from bookmarkap_bookmark where id = 1 -> id가 1인 bookmarkap_bookmark 데이터를 선택함
- select id, modify_date from blog_posts; => blog_posts의 정보들 중 id와 modify_date만 보여줌
- update blog_posts set modify_date=datetime(modify_date,'-3 month') where id=1; 
-> blog_posts의 id=1인 데이터의 modify_date를 현재 날짜보다 3달 앞당긴 날짜로 바꿈